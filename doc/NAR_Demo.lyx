#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 4cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nghia's Augmented Reality (NAR) demo
\begin_inset Newline newline
\end_inset

System documentation (work in progress)
\end_layout

\begin_layout Author
Nghia Ho
\end_layout

\begin_layout Date
5th Feb 2012
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename screenshot1.png
	lyxscale 50
	width 75col%

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
I wrote NAR Demo as an improvement to MarkerlessARDemo.
 MarkerlessARDemo was my first attempt at markerless augmented reality (AR).
 At the time I wanted to learn how an AR system worked.
 One of the main problems with MarkerlessARDemo was its reliant on CUDA,
 which I happened to be learning at the time.
 I have since then removed any GPU dependent code and made numerous changes.
 Overall NAR Demo works better, has less ambiguous parameters to tune, and
 cleaner code.
 I want to make NAR Demo easy enough for a beginner to poke around and understan
d the system.
 I hope someone will find it useful in their adventures into the AR field.
\end_layout

\begin_layout Standard
In this document I will be describing how NAR works and what makes it tick.
 I'll be focusing on the computer vision aspect of NAR and not on the code.
 The aim is to give you a good idea of how things work without bogging you
 down with the nitty gritty implementation details.
 For that you can just look at the code.
 
\end_layout

\begin_layout Standard
I assume the reader has some familiarity with common algorithms found in
 computer vision, such as 
\end_layout

\begin_layout Itemize
Gaussian blurring
\end_layout

\begin_layout Itemize
Homography
\end_layout

\begin_layout Itemize
K-means clustering
\end_layout

\begin_layout Itemize
Optical flow
\end_layout

\begin_layout Standard
This document is 
\lang british
organised
\lang english
 as follows, Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:System-architecture"

\end_inset

 will give an overview of the system architecture, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:KeyPointThread"

\end_inset

 talks about the keypoint detection process, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:ExtractFeatureThread"

\end_inset

 the feature descriptor used, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:NAR::DoWork"

\end_inset

the detection and pose estimation process, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Learning-the-AR"

\end_inset

 talks about how the AR object is learnt for detection, and some final words
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Some-final-words"

\end_inset

 to wrap things up.
\end_layout

\begin_layout Section
System architecture
\begin_inset CommandInset label
LatexCommand label
name "sec:System-architecture"

\end_inset


\end_layout

\begin_layout Standard
NAR is implemented as a multi-threaded system.
 This is required to achieve real-time frame rates, or close to it.
 This comes at a small cost of latency/lag between when the frame arrives
 and when it gets processed.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NAR-thread-flow"

\end_inset

 shows a flow chart of the threading system used.
 The work flow is typical of what you might find in a markerless augmented
 reality system, that is
\end_layout

\begin_layout Enumerate
Detect suitable keypoint features in the image
\end_layout

\begin_layout Enumerate
Extract a feature vector at each keypoint
\end_layout

\begin_layout Enumerate
Match the features 
\end_layout

\begin_layout Enumerate
Filter out the bad matches
\end_layout

\begin_layout Enumerate
Perform pose estimation on the good matches
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename NAR_arch.eps
	lyxscale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
NAR thread flow chart
\begin_inset CommandInset label
LatexCommand label
name "fig:NAR-thread-flow"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Step 1 is done in KeyPointThread, step 2 inside ExtractFeatureThread and
 steps 3 to 5 are inside the NAR::DoWork function.
 There are a total of 8 threads, VideoThread + KeyPointThread + ExtractThread
 + a thread that calls the NAR::DoWork function.
 The system will run in real-time provided each thread runs at least or
 faster than the incoming video frame per second, and provided you have
 a multi-core CPU.
 I'll briefly mention the first two stages in the threading pipeline, with
 the rest in greater detail in the later sections.
 
\end_layout

\begin_layout Subsection
VideoThread
\end_layout

\begin_layout Standard
This thread's job is simply to grab images from the webcam/camera and send
 it off to the main NAR thread.
\end_layout

\begin_layout Subsection
NAR::AddNewJob
\end_layout

\begin_layout Standard
The NAR thread manages the internal KeyPointThread and ExtractFeatureThread.
 The entry point to the NAR thread class is the NAR::AddNewJob function,
 which accepts an incoming images.
 NAR::AddNewJob creates 3 different resized images from the original image.
 This is done to detect features at different resolution levels.
 The default setting uses an image scaling factor of 0.75.
 The 3 images are produced using the follow equation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
new\, size=original\, size\times0.75^{k},\, k=\mbox{[0,1,2\ensuremath{]}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
As an example, a 640x480 will result in following 3 images.
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
k
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
image size
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
640x480
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
480x360
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
360x270
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Each resized image is sent to the respective KeyPointThread.
\end_layout

\begin_layout Section
KeyPointThread
\begin_inset CommandInset label
LatexCommand label
name "sec:KeyPointThread"

\end_inset


\end_layout

\begin_layout Standard
The KeyPointThread's job is to detect good features in the incoming image.
 A good keypoint feature should be robust to noise (a must on crappy webcams)
 and repeatable for a wide range of viewpoints.
 I chose the Difference of Gaussian (DOG), an approximation to the Laplacian,
 because it has these properties.
 I've tried FAST 
\begin_inset CommandInset citation
LatexCommand cite
key "rosten_2006_machine"

\end_inset

 but it seems too susceptible to noise, though having said that I have not
 done a thorough test.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Keypoint-detection-steps"

\end_inset

 shows the keypoint detection steps, most of the idea is borrowed from SIFT
 
\begin_inset CommandInset citation
LatexCommand cite
key "SIFT"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename KeyPointThread.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Keypoint detection steps
\begin_inset CommandInset label
LatexCommand label
name "fig:Keypoint-detection-steps"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Difference of Gaussian (DOG)
\end_layout

\begin_layout Standard
The DOG is the difference between two Gaussian blurred images, expressed
 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
DOG=G\left(image,\sigma_{1}\right)-G\left(image,\sigma2\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where G is the Gaussian blurring function of an image by some sigma value.
 The two sigma values I used are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{array}{c}
\sigma_{1}=1.6\\
\sigma_{2}=1.6*1.6=2.56
\end{array}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
I chose 1.6 based on SIFT's pre blurring value, the second sigma is based
 on a Wikipedia article (
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Difference_of_Gaussians
\end_layout

\end_inset

) stating a ratio of 1.6 is a good approximation to the Laplacian.
 Larger sigma values will detect larger blobs but at a cost of an increase
 in run time.
 Since we are detecting a fixed size blob at different image resolutions
 we end up detecting large blobs at the lower resolution images.
 Each pixel in the DOG image is examined to see if it passes all the tests
 shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Keypoint-detection-steps"

\end_inset

.
\end_layout

\begin_layout Subsection
Contrast test
\end_layout

\begin_layout Standard
The contrast test checks to see if the absolute pixel value in the DOG image
 is above a certain threshold, expressed as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\left|DOG(x,y)\right|>t
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\left|.\right|$
\end_inset

 is the absolute value function.
 The DOG values can be thought of as contrast values of blob features.
 A low value means low contrast, which indicates a weak blob feature, and
 hence will fail the test.
 Since this is the cheapest test it is performed first.
 I use a 
\begin_inset Formula $t=7.0$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Finding extrema
\end_layout

\begin_layout Standard
This step looks at the 3x3 
\lang british
neighbouring
\lang english
 pixels to see if the pixel stands out, either lower or greater than all
 the 
\lang british
neighbouring
\lang english
 values.
 If it meets this requirement then it is an extrema.
 An example of a 3x3 minima/maxima extrema is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-3x3"

\end_inset

.
 The centre value is compared to the 8 neighbouring pixel values.
 In practice the values are floats.
 This is different to SIFT, where the extrema is done in scale space as
 well, resulting in a 3D grid comparison.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename minima_maxima.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example of 3x3 minima/maxima extrema pixels
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-3x3"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Edge test
\end_layout

\begin_layout Standard
The edge test calculates the principal curvature at DOG(x,y) and determines
 whether the feature is too 'edge' like using the ratio of eigenvalue values,
 but uses a maths trick so that the eigenvalues don't have to be explicitly
 computed.
 The full eigenvalue calculation involves nasty square roots.
 If a pixel position is deemed to edge like it is skipped, because they
 are not stable features.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Edge-test"

\end_inset

 shows an image exhibiting a strong edge and the location of interest to
 do the edge test.
 I added noise to make it easier to see the white part of the image.
 The red circle in the Difference of Gaussian image is the location of interest.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DOG_edge/DOG_curvature.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Edge test using 3x3 region
\begin_inset CommandInset label
LatexCommand label
name "fig:Edge-test"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Let W be the 3x3 values indexed by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
W=\left[\begin{array}{ccc}
a & b & c\\
d & e & f\\
g & h & i
\end{array}\right]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The edge test calculation for the 3x3 window is as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{xx}=d-2e+f
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{yy}=b-2e+h
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x1}=\left(c-a\right)/2
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x2}=\left(i-h\right)/2
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{xy}=\left(D_{x2}-D_{x1}\right)/2
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The above quantities are the second order derivatives using finite differencing.
 The ratio of the eigenvalues is calculated as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
trace=D_{xx}+D_{yy}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
det=D_{xx}D_{yy}-D_{xy}D_{xy}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ratio=trace^{2}/det
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ratio>threshold?
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If the ratio exceeds a threshold it is deemed an edge and the pixel is skipped.
 The threshold used is 12.1.
 If 
\begin_inset Formula $det$
\end_inset

 is less than or equal to zero then it is not an extrema location and skipped
 as well.
 This happens when the sub-pixel localistion step does not converge correctly,
 which is not often .The example in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Edge-test"

\end_inset

 has a ratio of -0.13220 and is thus not an extrema.
 The calculation shown is very similar to the one used to calculate Harris
 corner features 
\begin_inset CommandInset citation
LatexCommand cite
key "HARRIS_CORNER"

\end_inset

, but uses the 2x2 Hessian matrix instead.
\end_layout

\begin_layout Subsection
Sub-pixel localisation
\end_layout

\begin_layout Standard
The sub-pixel localisation step refines the integer (x,y) position into
 floating point position using gradient descent.
 This is important when detecting keypoints at the lower resolution images
 because any errors in the extrema's position are magnified when scaling
 back up to the original image resolution.
 The extrema's sub-pixel location is found using basic gradient descent
 with a learning rate of 0.5.
 The cost function that gets minimised is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
cost=dx^{2}+dy^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where dx and dy are the first order derivatives at DOG(x,y).
 The derivative at DOG(x,y) is evaluated using the following finite differencing
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
dx=DOG\left(x+0.1,y\right)-DOG\left(x-0.1,y\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
dy=DOG\left(x,y+0.1\right)-DOG\left(x,y-0.1\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The gradient descent requires evaluating pixels at fractional position using
 bilinear interpolation.
 The gradient descent generally converges quickly.
 Sub-pixel localisation is only used when the input image is smaller than
 the original image size.
 I didn't see any point in doing it for the original size image since it's
 theoretically accurate to within a pixel already, and would add an unnecessary
 computation burden since more features are detected at the original image
 size.
\end_layout

\begin_layout Section
ExtractFeatureThread
\begin_inset CommandInset label
LatexCommand label
name "sec:ExtractFeatureThread"

\end_inset


\end_layout

\begin_layout Standard
This thread extracts a feature vector at the (x,y) position found by the
 KeyPointThread.
 I used my own home brew rotation invariant binary feature patch, having
 a length of 64 bits (8 bytes) per keypoint.
 The feature extraction process is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extracting-feature-vector"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ExtractFeature.eps
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Extracting feature vector
\begin_inset CommandInset label
LatexCommand label
name "fig:Extracting-feature-vector"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For a given keypoint (x,y) position, an orientation is calculated using
 pixels within a circle of radius 15.
 It's actually a circle within a 32x32 patch, but the circle doesn't centre
 perfectly within the patch.
 For more information on the orientation calculation please see my blog
 post 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://nghiaho.com/?p=1198
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The orientation is used to extract a 32x32 rotated patch, which is then
 sampled every 4 pixels to produce a down sampled 8x8 patch.
 In practice, the rotation and downsampling can be combined into one step
 for efficiency.
 The average intensity of the patch is then calculated and used to binarise
 the image by thresholding at this value.
 The final feature vector consists of 64 bits.
 The 64 bit length was carefully chosen to allow the use of the POPCNT assembly
 instruction found in CPUs supporting SSE4.
 The instruction counts the numbers of 1s in a 64 bit number in a single
 operation.
 
\end_layout

\begin_layout Section
NAR::DoWork
\begin_inset CommandInset label
LatexCommand label
name "sec:NAR::DoWork"

\end_inset


\end_layout

\begin_layout Standard
This function is where the magic happens.
 It's job is to perform pose estimation to determine the 3D pose of the
 AR object.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NAR::DoWork"

\end_inset

 outlines the steps involved.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DoWork.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
NAR::DoWork process
\begin_inset CommandInset label
LatexCommand label
name "fig:NAR::DoWork"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature matching
\end_layout

\begin_layout Standard
This uses the extracted features from the three ExtractFeatureThread and
 matches them against the AR object's feature (extracted beforehand).
 To speed up matching I used a hierarchical K-means tree, internally just
 called a K-tree.
 If you are familiar with K-means, the K-tree partitions the data by splitting
 it into 2 centroids recursively for a fixed depth.
 Searches are performed just like in a binary tree by comparing which centroid
 is closer to the query and going down the tree.
 The final leaf node will contain a handful of features that have to be
 searched linearly, but this is okay because there's only a handful of them
 to go through.
 
\end_layout

\begin_layout Standard
The trade off for a deeper tree is accuracy.
 My implementation does not traverse back up the tree to visit other potential
 nodes like in a standard KD-tree implementation.
 I've found in practice this is not required.
 
\end_layout

\begin_layout Standard
Some of the matches will not be one to one, that is more than one query
 feature can point to the same feature in the K-tree.
 To resolve this conflict, the one with the lowest matching score is kept.
 Here, a low score means a good match, using Hamming distance metric.
\end_layout

\begin_layout Subsection
Filtering matches by orientation
\end_layout

\begin_layout Standard
A lot of the initial matches will be bad matches that need to be pruned
 out.
 We can take advantage of the feature's orientation value to do this.
 I borrowed this idea from 
\begin_inset CommandInset citation
LatexCommand cite
key "AR_PHONE_1"

\end_inset

.
 Remember back that I use a rotation invariant feature that has an orientation
 associated with it.
 This means the relative angle between matches are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
relative\, angle=ShortestAngle\left(query,\, match\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The ShortestAngle is a function that returns the shortest angle between
 A and B, and is between 0 and 180 degrees.
 The angles are the accumulated into a histogram of 36 bins (every 10 degrees).
 Matches in the top two bins are kept.
 I chose the top two instead of the top to allow for angular errors in the
 binning processing.
\end_layout

\begin_layout Subsection
Homography with optical flow assist
\end_layout

\begin_layout Standard
After filtering the matches by the orientation, they are filtered further
 using the homography.
 RANSAC is used to find the best homography between the matches.
 However, I've observed that using matches from the DOG features only are
 not enough to produce a stable homography.
 It tends to jitter even when the AR object is stationary.
 To remedy this, additional matches from optical flow tracks are added.
 The optical flow tracks are initialised in the previous frame and tracked
 in the current frame using the popular Lucas-Kanade optical flow tracker
 
\begin_inset CommandInset citation
LatexCommand cite
key "Lucas81aniterative"

\end_inset

 found in OpenCV.
 The extra matches improve stability a lot.
\end_layout

\begin_layout Subsection
Pose estimation
\end_layout

\begin_layout Standard
By this stage we should have filtered out most, if not all, of the bad matches
 and are only left with good matches.
 The pose is estimated from these matches using the Robust Planar Pose (RPP)
 algorithm by 
\begin_inset CommandInset citation
LatexCommand cite
key "RPP"

\end_inset

.
 This algorithm does not incorporate any outlier filtering, hence why it's
 very important to give it clean data.
 The algorithm finds the best pose estimate given 2D to 3D correspondences.
 The 3D points here are the features detected in the AR object, with z=0.
 The x,y positions are normalised in a way such that (0,0) is at the centre
 of the image and the image width is length 1.0, the height is scaled so
 that the aspect ratio does not change.
 The pose estimated is relative to this normalised coordinate space.
\end_layout

\begin_layout Subsection
Smoothing pose using alpha-beta filter
\end_layout

\begin_layout Standard
This step takes the estimated pose from the RPP and smooths it using an
 alpha-beta filter (
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Alpha_beta_filter
\end_layout

\end_inset

).
 It is similar to a Kalman filter but less complex and less parameters to
 tune, the alpha and beta values.
 The alpha controls how responsive the filter is to new pose input (position
 and rotation) and the beta controls the response to new velocity input.
 The alpha/beta gains range from [0,1].
 In general.
 the lower the gains are the smoother the pose estimates but the less responsive
 it is to new input.
 I don't actually use velocity because I found it unreliable, especially
 when there is motion blur.
 My alpha-beta filter update equation is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r_{t}=X_{t}-\hat{X}_{t-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{X}_{t}=\hat{X}_{t-1}+\alpha r_{t}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{X}_{t}$
\end_inset

 is the current smoothed out pose estimate.
 
\begin_inset Formula $\hat{X}_{t-1}$
\end_inset

 is the previous estimate, 
\begin_inset Formula $X_{t}$
\end_inset

 is the raw pose input, 
\begin_inset Formula $r_{t}$
\end_inset

 is the residual, and 
\begin_inset Formula $\alpha$
\end_inset

 is the gain.
 The pose is a 6D vector (x, y, z, yaw, pitch, roll).
 The smoothing is used to reduce jittering of the AR object.
 This step is not required if the pose estimation step mentioned previously
 is very stable.
\end_layout

\begin_layout Subsection
Update search region
\end_layout

\begin_layout Standard
Being able to predict where the AR object might be in the next frame is
 important for good performance.
 It allows us to speed up feature matching and reduce bad matches because
 we don't have to process every pixel.
 Instead of using an active search region to predict where the AR object
 is in the next frame, say using results from the alpha-beta filter (or
 Kalman filter), I use a simpler approach instead.
 
\end_layout

\begin_layout Standard
My approach assumes the AR object is mostly stationary and will restrict
 keypoint detection to a region in the image, formed by a bounding box around
 the rectangular contour of the AR object in the previous frame.
 This works well if the movement is not too large, because the search region
 will shift with the AR object as it moves.
 There is an option in the system to increase the search region by padding
 the borders to cater for larger movements.
 An illustrative example is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:search-region"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename search-region.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Simple search region used by NAR
\begin_inset CommandInset label
LatexCommand label
name "fig:search-region"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Update optical flow tracks
\end_layout

\begin_layout Standard
This step will add optical flow tracks or reset the optical flow tracks
 depending on the situation.
 Optical flows are reseted often to prevent them from going 'stale' due
 to drifts or bad tracking, which will mess up pose estimation.
 When that happens new fresh optical flow tracks are added.
 The heuristics for this process is illustrated in the flow chart in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Optical-flow-update"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename optical_flow_update.eps
	lyxscale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Optical flow update
\begin_inset CommandInset label
LatexCommand label
name "fig:Optical-flow-update"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Optical flow tracks are reset if the count exceeds 16.
 The count is incremented if the system detects significant movement from
 the AR object.
 This is based on my observation that optical flow works well when tracking
 an object that doesn't move too fast, before motion blur kicks in, but
 starts to degrade quickly in the opposite case.
 When the object is moving fast the tracked optical flow vectors can track
 incorrectly.
 Optical flow tracks are also reseted if more than a 30% of the original
 numbers of track have been lost over time (due to tracking failure).
\end_layout

\begin_layout Section
Learning the AR object for recognition
\begin_inset CommandInset label
LatexCommand label
name "sec:Learning-the-AR"

\end_inset


\end_layout

\begin_layout Standard
Up until now I've been focusing on the low level steps involved in detecting
 and estimating the pose of the AR object.
 I will now discuss how the AR object is learnt and features extracted.
 This is done when the program starts up and is a fast process.
\end_layout

\begin_layout Standard
My method for learning the AR object is inspired by 
\begin_inset CommandInset citation
LatexCommand cite
key "FERN_1"

\end_inset

.
 The author presents a method where by a planar object is warped with random
 affine transforms, producing virtually unlimited appearances from different
 viewpoint.
 The idea is to learn the object by observing it from as much view as possible.
 At each view simple features are extracted.
 They used a DOG at 3 scale levels to detect keypoints and random binary
 comparisons to produce a 300 bit feature vector.
 The feature vector is not rotation, scale, or viewpoint invariant but is
 illumination invariant.
\end_layout

\begin_layout Standard
My approach is similar but instead of warping with a random affine, my affine
 is parameterised by three rotation angles: yaw, pitch, roll (always 0).
 Sscale is done implicitly by resizing the image.
 The steps to derive the 2D affine matrix are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
R_{4x4}=MakeRotationMatrix\left(yaw,pitch,roll\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
T=T_{centre}R_{4x4}T_{origin}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
A_{2x3}=\left[\begin{array}{cccc}
X & X & - & X\\
X & X & - & X\\
- & - & - & -\\
0 & 0 & 0 & 1
\end{array}\right]_{T}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $MakeRotationMatrix$
\end_inset

 is a function that returns a 4x4 rotation matrix from yaw, pitch, roll,
 the matrix is padded with zeros where required.
 
\begin_inset Formula $T_{origin}$
\end_inset

 is a 4x4 matrix that shifts the origin to the centre of the image, 
\begin_inset Formula $T_{centre}$
\end_inset

 is a 4x4 matrix that undoes the shift by 
\begin_inset Formula $T_{origin}$
\end_inset

.
 
\begin_inset Formula $A_{2x3}$
\end_inset

 is the 2x3 affine matrix, by taking values marked 
\begin_inset Formula $X$
\end_inset

 in matrix 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Standard
The three rotation angles are sampled at fixed intervals as follows, all
 values are in degrees:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
angle
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
start
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
end
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
angle step
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
yaw
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
pitch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
roll
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
I prefer this method over random values because I know I'll get the same
 results every time and it samples the viewpoint space uniformly.
 An example of affine warping is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-affine"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename warp1.png
	lyxscale 50
	width 50col%

\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Graphics
	filename warp2.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example of affine warping
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-affine"

\end_inset

 parameterised by yaw, pitch, roll
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For each warped image, DOG keypoints are detected and feature vectors extracted
 just as described previously in the KeyPointThread and ExtractFeatureThread.
 However there is one minor difference.
 The orientation calculation for the patch, to get the rotation invariance,
 has an angle ambiguity.
 There are two valid angles, the second one being out by 180 degrees.
 So for each patch two binary vectors are stored.
\end_layout

\begin_layout Section
Some final words
\begin_inset CommandInset label
LatexCommand label
name "sec:Some-final-words"

\end_inset


\end_layout

\begin_layout Standard
That brings us to end of the document.
 We've covered the threading pipeline, keypoint detection, feature extraction,
 feature matching and pose estimation.
 Some sections I've skimmed through while others have been given more time.
 This choice was made based on whether I felt a topic was better explained
 with figures and equations, or adequately explained with a few passing
 sentences, and have the reader find information about it somewhere else.
 I chose to expand on the keypoint detection step because the sub topics
 are often found in computer vision but lack adequate resources for beginners
 eg.
 papers, tutorials, blogs.
 I might continue to keep expanding the topics if there is a demand for
 them and maybe treat them as independent tutorial blogs.
\end_layout

\begin_layout Standard
So far I've only focused on the technical aspect of the AR system.
 However, the most important aspect from a user point of view is the actual
 performance of the system.
 The performance includes
\end_layout

\begin_layout Enumerate
Real-time run speed
\end_layout

\begin_layout Enumerate
Detection robustness of the AR object in difficult views
\end_layout

\begin_layout Enumerate
Tracking robustness of the AR object during motion
\end_layout

\begin_layout Enumerate
Quality of pose estimation, jitter reduction
\end_layout

\begin_layout Standard
The four points are general and not application specific but are typical
 of the challenges facing any AR system.
 On a desktop computer it is quite easy to achieve real-time speed but the
 trend these days is to get AR systems working on mobile devices 
\begin_inset CommandInset citation
LatexCommand cite
key "AR_MOBILE"

\end_inset

 and even towards tracking of multiple objects 
\begin_inset CommandInset citation
LatexCommand cite
key "taylor_2009_multiple"

\end_inset

.
 Due to the limited processing power of smart phone, as compared to a desktop
 computer, they have to utilise clever tricks and efficient algorithms to
 get the job done.
 Because of this, expect a lot of cool innovate algorithms stemming from
 this.
\end_layout

\begin_layout Standard
Anyway, I hope I've explain how NAR works well enough.
 Now go fourth and build your own AR system!
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "papers"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
